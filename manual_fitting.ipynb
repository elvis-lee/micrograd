{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2746ee8",
   "metadata": {},
   "source": [
    "Learning  = Data + Model + Loss + Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539b93d",
   "metadata": {},
   "source": [
    "# Manual Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0710804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from micrograd.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f3e2286",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Value(0); y1 = Value(2)\n",
    "x2 = Value(1); y2 = Value(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cea4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration 0#####\n",
      "==forward pass\n",
      "L=Value(data=26.0, grad=0)\n",
      "==backward pass\n",
      "Value(data=1, grad=-6.0)\n",
      "Value(data=-2, grad=-10.0)\n",
      "==gradient descent\n",
      "Value(data=1.6, grad=-6.0)\n",
      "Value(data=-1.0, grad=-10.0)\n",
      "##### iteration 1#####\n",
      "==forward pass\n",
      "L=Value(data=14.180000000000001, grad=0)\n",
      "==backward pass\n",
      "Value(data=1.6, grad=-4.4)\n",
      "Value(data=-1.0, grad=-7.4)\n",
      "==gradient descent\n",
      "Value(data=2.04, grad=-4.4)\n",
      "Value(data=-0.2599999999999999, grad=-7.4)\n",
      "##### iteration 2#####\n",
      "==forward pass\n",
      "L=Value(data=7.737999999999998, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.04, grad=-3.2199999999999998)\n",
      "Value(data=-0.2599999999999999, grad=-5.4799999999999995)\n",
      "==gradient descent\n",
      "Value(data=2.362, grad=-3.2199999999999998)\n",
      "Value(data=0.28800000000000003, grad=-5.4799999999999995)\n",
      "##### iteration 3#####\n",
      "==forward pass\n",
      "L=Value(data=4.226721999999999, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.362, grad=-2.3499999999999996)\n",
      "Value(data=0.28800000000000003, grad=-4.061999999999999)\n",
      "==gradient descent\n",
      "Value(data=2.597, grad=-2.3499999999999996)\n",
      "Value(data=0.6941999999999999, grad=-4.061999999999999)\n",
      "##### iteration 4#####\n",
      "==forward pass\n",
      "L=Value(data=2.3125555400000004, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.597, grad=-1.7088)\n",
      "Value(data=0.6941999999999999, grad=-3.0146)\n",
      "==gradient descent\n",
      "Value(data=2.76788, grad=-1.7088)\n",
      "Value(data=0.99566, grad=-3.0146)\n",
      "##### iteration 5#####\n",
      "==forward pass\n",
      "L=Value(data=1.2687660836, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.76788, grad=-1.2364600000000001)\n",
      "Value(data=0.99566, grad=-2.2408)\n",
      "==gradient descent\n",
      "Value(data=2.891526, grad=-1.2364600000000001)\n",
      "Value(data=1.21974, grad=-2.2408)\n",
      "##### iteration 6#####\n",
      "==forward pass\n",
      "L=Value(data=0.6993268951780003, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.891526, grad=-0.8887340000000004)\n",
      "Value(data=1.21974, grad=-1.6689940000000003)\n",
      "==gradient descent\n",
      "Value(data=2.9803994, grad=-0.8887340000000004)\n",
      "Value(data=1.3866394, grad=-1.6689940000000003)\n",
      "##### iteration 7#####\n",
      "==forward pass\n",
      "L=Value(data=0.38842555316890026, grad=0)\n",
      "==backward pass\n",
      "Value(data=2.9803994, grad=-0.6329612000000004)\n",
      "Value(data=1.3866394, grad=-1.2463218000000005)\n",
      "==gradient descent\n",
      "Value(data=3.04369552, grad=-0.6329612000000004)\n",
      "Value(data=1.51127158, grad=-1.2463218000000005)\n",
      "##### iteration 8#####\n",
      "==forward pass\n",
      "L=Value(data=0.21845487529905325, grad=0)\n",
      "==backward pass\n",
      "Value(data=3.04369552, grad=-0.4450329000000002)\n",
      "Value(data=1.51127158, grad=-0.9337613200000001)\n",
      "==gradient descent\n",
      "Value(data=3.08819881, grad=-0.4450329000000002)\n",
      "Value(data=1.604647712, grad=-0.9337613200000001)\n",
      "##### iteration 9#####\n",
      "==forward pass\n",
      "L=Value(data=0.12532334533716571, grad=0)\n",
      "==backward pass\n",
      "Value(data=3.08819881, grad=-0.30715347800000004)\n",
      "Value(data=1.604647712, grad=-0.702505766)\n",
      "==gradient descent\n",
      "Value(data=3.1189141578000004, grad=-0.30715347800000004)\n",
      "Value(data=1.6748982886000001, grad=-0.702505766)\n"
     ]
    }
   ],
   "source": [
    "w = Value(1)\n",
    "b = Value (-2)\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"##### iteration \" + str(i) + \"#####\")\n",
    "    print(\"==forward pass\")\n",
    "    L = ((x1*w+b - y1)**2 + (x2*w+b - y2)**2)/2\n",
    "    print(\"L=\" + str(L))\n",
    "\n",
    "    print(\"==backward pass\")\n",
    "    w.grad = 0;b.grad = 0\n",
    "    x1.grad = 0; y1.grad = 0; x2.grad = 0; y2.grad = 0\n",
    "    L.backward()\n",
    "    print(w)\n",
    "    print(b)\n",
    "\n",
    "    print(\"==gradient descent\")\n",
    "    w.data += -lr * w.grad\n",
    "    b.data += -lr * b.grad\n",
    "    print(w)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384d5675",
   "metadata": {},
   "source": [
    "# Replicate in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36dddd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220b18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0.0], [1.0]])\n",
    "y = torch.tensor([[2.0], [5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96cd6139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration 0#####\n",
      "==forward pass\n",
      "L=tensor(26., grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[1.6000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n",
      "##### iteration 1#####\n",
      "==forward pass\n",
      "L=tensor(14.1800, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[1.6000]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.0400]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2600], requires_grad=True)\n",
      "##### iteration 2#####\n",
      "==forward pass\n",
      "L=tensor(7.7380, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.0400]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2600], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.3620]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2880], requires_grad=True)\n",
      "##### iteration 3#####\n",
      "==forward pass\n",
      "L=tensor(4.2267, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.3620]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2880], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.5970]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6942], requires_grad=True)\n",
      "##### iteration 4#####\n",
      "==forward pass\n",
      "L=tensor(2.3126, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.5970]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6942], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.7679]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9957], requires_grad=True)\n",
      "##### iteration 5#####\n",
      "==forward pass\n",
      "L=tensor(1.2688, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.7679]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9957], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.8915]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.2197], requires_grad=True)\n",
      "##### iteration 6#####\n",
      "==forward pass\n",
      "L=tensor(0.6993, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.8915]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.2197], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[2.9804]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.3866], requires_grad=True)\n",
      "##### iteration 7#####\n",
      "==forward pass\n",
      "L=tensor(0.3884, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[2.9804]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.3866], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[3.0437]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.5113], requires_grad=True)\n",
      "##### iteration 8#####\n",
      "==forward pass\n",
      "L=tensor(0.2185, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[3.0437]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.5113], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[3.0882]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.6046], requires_grad=True)\n",
      "##### iteration 9#####\n",
      "==forward pass\n",
      "L=tensor(0.1253, grad_fn=<MseLossBackward0>)\n",
      "==backward pass\n",
      "Parameter containing:\n",
      "tensor([[3.0882]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.6046], requires_grad=True)\n",
      "==gradient descent\n",
      "Parameter containing:\n",
      "tensor([[3.1189]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1.6749], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(1, 1)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(torch.tensor([[1.0]]))\n",
    "    model.bias.copy_(torch.tensor([-2.0]))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"##### iteration \" + str(i) + \"#####\")\n",
    "    print(\"==forward pass\")\n",
    "    L = loss(model(x), y)\n",
    "    print(\"L=\" + str(L))\n",
    "\n",
    "    print(\"==backward pass\")\n",
    "    model.zero_grad()\n",
    "    L.backward()\n",
    "    print(model.weight)\n",
    "    print(model.bias)\n",
    "\n",
    "    print(\"==gradient descent\")\n",
    "    optimizer.step()\n",
    "    print(model.weight)\n",
    "    print(model.bias)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micrograd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
